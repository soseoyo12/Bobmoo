#!/usr/bin/env python3
"""
PDF ì²˜ë¦¬ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""

import asyncio
import logging
import sys
from web_scraper import WebScraper

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('pdf_test.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)

async def test_pdf_detection():
    """PDF ë§í¬ ê°ì§€ í…ŒìŠ¤íŠ¸"""
    print("ğŸ” PDF ë§í¬ ê°ì§€ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    scraper = WebScraper()
    
    # í…ŒìŠ¤íŠ¸ìš© HTML (PDF ë§í¬ê°€ ìˆëŠ” ê²½ìš°)
    test_html = """
    <html>
    <body>
        <h1>ì‹ë‹¨í‘œ</h1>
        <a href="/menu/2024-10-01.pdf">10ì›” 1ì¼ ì‹ë‹¨í‘œ</a>
        <a href="/menu/2024-10-02.pdf">10ì›” 2ì¼ ì‹ë‹¨í‘œ</a>
        <iframe src="/menu/current.pdf"></iframe>
    </body>
    </html>
    """
    
    base_url = "https://example.com"
    pdf_links = scraper.detect_pdf_links(test_html, base_url)
    
    print(f"ë°œê²¬ëœ PDF ë§í¬: {len(pdf_links)}ê°œ")
    for link in pdf_links:
        print(f"  - {link}")
    
    return len(pdf_links) > 0

async def test_pdf_processing():
    """PDF ì²˜ë¦¬ ì „ì²´ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ“„ PDF ì²˜ë¦¬ ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    scraper = WebScraper()
    
    # ì‹¤ì œ PDF URLë¡œ í…ŒìŠ¤íŠ¸ (ì˜ˆ: ê³µê°œëœ PDF íŒŒì¼)
    test_url = "https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf"
    
    try:
        print(f"í…ŒìŠ¤íŠ¸ URL: {test_url}")
        content = await scraper.scrape_content(test_url)
        
        print(f"ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(content)} ë¬¸ì")
        print(f"í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°: {content[:200]}...")
        
        return True
        
    except Exception as e:
        print(f"PDF ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        return False

async def test_html_fallback():
    """HTML í´ë°± í…ŒìŠ¤íŠ¸"""
    print("\nğŸŒ HTML í´ë°± í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    scraper = WebScraper()
    
    # PDFê°€ ì—†ëŠ” ì¼ë°˜ ì›¹ì‚¬ì´íŠ¸ í…ŒìŠ¤íŠ¸
    test_url = "https://httpbin.org/html"
    
    try:
        print(f"í…ŒìŠ¤íŠ¸ URL: {test_url}")
        content = await scraper.scrape_content(test_url)
        
        print(f"ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(content)} ë¬¸ì")
        print(f"í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°: {content[:200]}...")
        
        return True
        
    except Exception as e:
        print(f"HTML í´ë°± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        return False

async def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print("ğŸ§ª PDF ì²˜ë¦¬ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘\n")
    
    # 1. PDF ë§í¬ ê°ì§€ í…ŒìŠ¤íŠ¸
    pdf_detection_success = await test_pdf_detection()
    
    # 2. HTML í´ë°± í…ŒìŠ¤íŠ¸ (PDF ì²˜ë¦¬ ì—†ì´)
    html_fallback_success = await test_html_fallback()
    
    # 3. PDF ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ (ì‹¤ì œ PDFê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ)
    # pdf_processing_success = await test_pdf_processing()
    
    print("\nğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
    print(f"  âœ… PDF ë§í¬ ê°ì§€: {'ì„±ê³µ' if pdf_detection_success else 'ì‹¤íŒ¨'}")
    print(f"  âœ… HTML í´ë°±: {'ì„±ê³µ' if html_fallback_success else 'ì‹¤íŒ¨'}")
    # print(f"  âœ… PDF ì²˜ë¦¬: {'ì„±ê³µ' if pdf_processing_success else 'ì‹¤íŒ¨'}")
    
    if pdf_detection_success and html_fallback_success:
        print("\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!")
        return True
    else:
        print("\nâŒ ì¼ë¶€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
        return False

if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)
