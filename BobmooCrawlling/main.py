#!/usr/bin/env python3
"""
ì›¹ ìŠ¤í¬ë˜í•‘ ë° Gemini APIë¥¼ ì‚¬ìš©í•œ ì‹ë‹¹ ë©”ë‰´ ì •ë³´ ì¶”ì¶œ í”„ë¡œê·¸ë¨
"""

import asyncio
import logging
import argparse
import sys
from web_scraper import WebScraper
from gemini_parser import GeminiParser
from file_manager import FileManager

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('scraper.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)

class MenuScraper:
    def __init__(self, output_dir: str = "output"):
        self.web_scraper = WebScraper()
        self.gemini_parser = GeminiParser()
        self.file_manager = FileManager(output_dir)
        self.logger = logging.getLogger(__name__)
    
    async def scrape_and_save(self, urls: list, school_name: str = "ì¸í•˜ëŒ€í•™êµ", wait_time: int = 3000):
        """
        ì—¬ëŸ¬ ì›¹ì‚¬ì´íŠ¸ë¥¼ ìŠ¤í¬ë˜í•‘í•˜ê³  JSONìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
        
        Args:
            urls (list): ìŠ¤í¬ë˜í•‘í•  URL ë¦¬ìŠ¤íŠ¸
            school_name (str): í•™êµ ì´ë¦„
            wait_time (int): í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì‹œê°„ (ë°€ë¦¬ì´ˆ)
        """
        try:
            all_cafeteria_data = {}  # ë‚ ì§œë³„ë¡œ ëª¨ë“  ì‹ë‹¹ ì •ë³´ë¥¼ ì €ì¥
            
            # ê° URLì—ì„œ ë°ì´í„° ì¶”ì¶œ
            for i, url in enumerate(urls, 1):
                self.logger.info(f"ìŠ¤í¬ë˜í•‘ ì‹œì‘ ({i}/{len(urls)}): {url}")
                
                # 1. HTML ìŠ¤í¬ë˜í•‘
                html_content = await self.web_scraper.scrape_html(url, wait_time)
                self.logger.info(f"HTML ìŠ¤í¬ë˜í•‘ ì™„ë£Œ ({i}/{len(urls)})")
                
                # 2. HTML ì •ë¦¬
                clean_text = self.web_scraper.clean_html(html_content)
                self.logger.info(f"HTML ì •ë¦¬ ì™„ë£Œ ({i}/{len(urls)})")
                
                # 3. Gemini APIë¡œ JSON ë³€í™˜
                json_data_list = self.gemini_parser.parse_html_to_json(clean_text, school_name)
                self.logger.info(f"Gemini API ë¶„ì„ ì™„ë£Œ ({i}/{len(urls)}) - {len(json_data_list)}ê°œ ë‚ ì§œì˜ ë°ì´í„° ì¶”ì¶œ")
                
                # 4. ë‚ ì§œë³„ë¡œ ì‹ë‹¹ ì •ë³´ ë³‘í•©
                for json_data in json_data_list:
                    date = json_data.get('date')
                    if date not in all_cafeteria_data:
                        all_cafeteria_data[date] = {
                            "date": date,
                            "school": school_name,
                            "cafeterias": []
                        }
                    
                    # ì‹ë‹¹ ì •ë³´ ë³‘í•©
                    merged_data = self.file_manager.merge_cafeteria_data(
                        all_cafeteria_data[date], 
                        json_data
                    )
                    all_cafeteria_data[date] = merged_data
            
            # 5. ê° ë‚ ì§œë³„ë¡œ JSON íŒŒì¼ ì €ì¥
            saved_files = []
            for date, json_data in all_cafeteria_data.items():
                filepath = self.file_manager.save_json_by_date(json_data, school_name)
                saved_files.append(filepath)
                self.logger.info(f"íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filepath}")
            
            return saved_files
            
        except Exception as e:
            self.logger.error(f"ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise

def main():
    parser = argparse.ArgumentParser(description='ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì‹ë‹¹ ë©”ë‰´ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ JSONìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.')
    parser.add_argument('urls', nargs='+', help='ìŠ¤í¬ë˜í•‘í•  ì›¹ì‚¬ì´íŠ¸ URLë“¤ (ì—¬ëŸ¬ ê°œ ê°€ëŠ¥)')
    parser.add_argument('--school', '-s', default='ì¸í•˜ëŒ€í•™êµ', help='í•™êµ ì´ë¦„ (ê¸°ë³¸ê°’: ì¸í•˜ëŒ€í•™êµ)')
    parser.add_argument('--output', '-o', default='output', help='ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: output)')
    parser.add_argument('--wait', '-w', type=int, default=3000, help='í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì‹œê°„(ë°€ë¦¬ì´ˆ) (ê¸°ë³¸ê°’: 3000)')
    
    args = parser.parse_args()
    
    # MenuScraper ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    scraper = MenuScraper(args.output)
    
    # ë¹„ë™ê¸° ì‹¤í–‰
    try:
        saved_files = asyncio.run(scraper.scrape_and_save(args.urls, args.school, args.wait))
        print(f"\nâœ… ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print(f"ğŸ“ ì €ì¥ëœ íŒŒì¼ ìˆ˜: {len(saved_files)}ê°œ")
        
        # ì €ì¥ëœ íŒŒì¼ ëª©ë¡ ì¶œë ¥
        print(f"\nğŸ“‹ ìƒˆë¡œ ìƒì„±ëœ íŒŒì¼ ëª©ë¡:")
        for file in saved_files:
            print(f"  - {file}")
        
        # ì „ì²´ ì €ì¥ëœ íŒŒì¼ ëª©ë¡ ì¶œë ¥
        all_files = scraper.file_manager.list_saved_files()
        if all_files:
            print(f"\nğŸ“‚ ì „ì²´ ì €ì¥ëœ íŒŒì¼ ëª©ë¡:")
            for file in all_files:
                print(f"  - {file}")
                
    except KeyboardInterrupt:
        print("\nâŒ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    main()
